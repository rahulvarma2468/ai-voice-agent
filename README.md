

# ðŸŽ™ AI Voice Conversation Agent

This is my personal build of an **AI-powered web voice assistant** â€” an app that listens to what I say, understands it, and talks back in a natural voice.  
Itâ€™s the *Day 13 milestone* of my **#30DaysOfAIVoiceents journey, where Iâ€™m steadily shaping this project into a polished, working product.


## ðŸš€ What This Project Does

- Hit **one record button**, speak, and let the agent process your words.
- Speech is converted to text (STT), sent to an LLM for generating a reply.
- The reply is displayed in a **chat bubble** *and* spoken back to you instantly (TTS).
- The UI keeps a full conversation history in the browser.

I wanted this to feel like chatting with a virtual buddy â€” minimal clicks, maximum flow.

***

## ðŸ§© How I Built It

**Frontend:**  
- Plain HTML, CSS & JavaScript (no frameworks â€” kept it light and fast).  
- Singleâ€‘toggle mic button with animated pulse while recording.  
- Chat bubble design for clear user/AI separation.  

**Backend:**  
- **FastAPI** (Python) handling audio uploads & AI responses.  
- Routes for processing audio and returning text + audio.  

**AI Services:**  
- **AssemblyAI** â€“ Speechâ€‘toâ€‘Text  
- **Google Gemini** â€“ Conversation generation  
- **Murf API** â€“ Textâ€‘toâ€‘Speech  



## ðŸ—º Quick Architecture


Browser (Record Audio)  
   â†“
FastAPI Backend
   â†“
[AssemblyAI] â†’ Converts Speech â†’ Text  
[Gemini API] â†’ Generates AI Reply  
[Murf API] â†’ Converts Text â†’ Voice  
   â†“
Browser (Show reply + Play audio)




## âœ¨ Features Iâ€™m Proud Of

- ðŸŽ¤ **Oneâ€‘Button Recording** â†’ Start/Stop in the same button.  
- ðŸ’¬ **Live Chat History** â†’ Keeps both my words and AIâ€™s words visible.  
- ðŸ”Š **Auto Audio Playback** â†’ I donâ€™t have to press play.  
- ðŸŽ¨ **Custom Background & Glass Look** â†’ My own style, no templates.  
- âš¡ **Responsive Flow** â†’ Few seconds from my voice to AIâ€™s reply.  



## ðŸ›  Running It on Your System

**1. Clone the repo**

git clone 
cd 


**2. Install the Python dependencies**

pip install -r requirements.txt


**3. Create a `.env` file** in the project root with:

MURF_API_KEY=your_murf_api_key
ASSEMBLYAI_API_KEY=your_assemblyai_key
GEMINI_API_KEY=your_gemini_key


**4. Start the FastAPI server**

uvicorn app:app --reload


**5. Open** `http://localhost:8000` in your browser.


## ðŸ”‘ Environment Variables

| Variable             | Purpose                  |
|----------------------|--------------------------|
| `MURF_API_KEY`       | Textâ€‘toâ€‘Speech            |
| `ASSEMBLYAI_API_KEY` | Speechâ€‘toâ€‘Text            |
| `GEMINI_API_KEY`     | LLM Response Generation   |





## ðŸ“š Why Iâ€™m Doing This

Iâ€™m building this project as part of my routine of my Daily Learnings  and my **#30DaysOfAIVoiceAgentshallenge.  
The aim is more than just writing code â€”  
itâ€™s about learning to **integrate AI services**, improve **UI for humans**, and prepare myself for **real-world AI product development**.

If youâ€™ve built something similar or have suggestions, drop me a message â€” I enjoy exchanging ideas with other builders. ðŸš€


ðŸ’¡ *Tomorrowâ€¦ I aim to make response times even faster and experiment with extra voice options.*

